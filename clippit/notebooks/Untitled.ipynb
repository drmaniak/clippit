{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a69fdd-1d44-401d-ae6e-c64cdc6970ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from PIL.JpegImagePlugin import JpegImageFile\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from transformers.models.clip.modeling_clip import CLIPOutput\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "def process_and_save_clip_embeddings(\n",
    "    output_dir: Path | str,\n",
    "    topk: int = 1,\n",
    "    shortest_edge: int = 224,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process Flickr30k dataset and save CLIP embeddings with topk similar captions.\n",
    "\n",
    "    Args:\n",
    "        output_dir: Directory to save the processed dataset\n",
    "        split: Dataset split ('train', 'test', 'validation')\n",
    "        topk: Number of most similar captions to keep per image\n",
    "        batch_size: Batch size for processing\n",
    "        device: Device to use for computation\n",
    "    \"\"\"\n",
    "    # Load CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)  # type: ignore\n",
    "    processor: CLIPProcessor = CLIPProcessor.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    )  # type: ignore\n",
    "\n",
    "    # Load Flickr dataset\n",
    "    flickr = load_dataset(\"nlphuji/flickr30k\")\n",
    "    dataset: DatasetDict = flickr[\"test\"]  # type: ignore\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Prepare output file\n",
    "\n",
    "    data_dict = {\"train\": [], \"test\": [], \"val\": []}\n",
    "\n",
    "    # Process dataset in batches\n",
    "    for b, row in enumerate(tqdm(dataset, desc=\"Processing Dataset\", total=len(dataset))):\n",
    "        image: JpegImageFile = row[\"image\"]\n",
    "        captions: list[str] = row[\"caption\"]\n",
    "        split: str = row[\"split\"]\n",
    "        width, height = image.size\n",
    "        image_id: int = int(row[\"img_id\"])\n",
    "        filename: str = row[\"filename\"]\n",
    "\n",
    "        # Pass the image & 5 captions to the CLIP Processor\n",
    "        vision_input: BatchEncoding = processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            size={\"shortest_edge\": shortest_edge},\n",
    "            padding=True,\n",
    "        ).to(device)\n",
    "\n",
    "        model_input: BatchEncoding = processor(\n",
    "            text=captions,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            size={\"shortest_edge\": shortest_edge},\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        ).to(device)\n",
    "\n",
    "        # Pass this input into CLIP to get outputs\n",
    "        model_output: CLIPOutput = model(**model_input)\n",
    "\n",
    "        # This obtains the CLS token for the image (batch_size, d_model=512)\n",
    "        image_output = model.get_image_features(**vision_input).squeeze()  # type: ignore\n",
    "\n",
    "        # We will now pick the top-k most similar captions\n",
    "        _, caption_indices = model_output[\"logits_per_image\"].topk(k=topk)\n",
    "        for idx in caption_indices[0].tolist():\n",
    "            data_row = {\n",
    "                \"img_embedding\": image_output.tolist(),\n",
    "                \"caption_text\": captions[idx],\n",
    "                \"img_id\": image_id,\n",
    "                \"filename\": filename,\n",
    "            }\n",
    "            # Append the row to the data_list for the corresponding split\n",
    "            data_dict[split].append(data_row)\n",
    "\n",
    "        if b >= 50:\n",
    "            break\n",
    "\n",
    "    print(\"Saving data to parquet files\")\n",
    "    # Once done with making lists, create dataframes, save as parquet\n",
    "    save_dataframe_parquet(\n",
    "        data_dict=data_dict, topk=topk, split=\"train\", output_dir=output_dir\n",
    "    )\n",
    "    save_dataframe_parquet(\n",
    "        data_dict=data_dict, topk=topk, split=\"val\", output_dir=output_dir\n",
    "    )\n",
    "    save_dataframe_parquet(\n",
    "        data_dict=data_dict, topk=topk, split=\"test\", output_dir=output_dir\n",
    "    )\n",
    "\n",
    "\n",
    "def save_dataframe_parquet(\n",
    "    data_dict: dict[str, Any], topk: int, split: str, output_dir: Path | str\n",
    "):\n",
    "    df = pd.DataFrame(data_dict[split])\n",
    "    filepath = Path(output_dir) / f\"flickr_{split}_top{topk}.parquet\"\n",
    "    df.to_parquet(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e9fc2a-df51-4a12-80a8-058c7de4dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Flicker30K(Dataset):\n",
    "    # TODO: Try swapping out clip tokenizer & text embeddings for GPT2's\n",
    "    def __init__(self, datafile: Path | str):\n",
    "        super().__init__()\n",
    "        datafile = Path(datafile)\n",
    "        if datafile.is_file() and datafile.suffix == \".parquet\":\n",
    "            self.dataset = pd.read_parquet(datafile)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No datafile found in {datafile}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_emb, cap, img_id = self.dataset.iloc[idx][\n",
    "            [\n",
    "                \"img_embedding\",\n",
    "                \"caption_text\",\n",
    "                \"img_id\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        img_emb = torch.tensor(img_emb)\n",
    "\n",
    "        return {\n",
    "            \"image_emb\": img_emb.to(\"cpu\", dtype=torch.float32),\n",
    "            \"caption\": cap,\n",
    "            \"img_id\": img_id,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233c24cf-0617-4b13-8a4f-c419d91c032f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = Path(\"../datafiles/\")\n",
    "output_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7d7712-8fb9-4910-8dc3-bfda8b597cd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Dataset:   0%|                          | 50/31014 [00:16<2:48:17,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_and_save_clip_embeddings(output_dir, 1, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4118152-d59c-4586-adec-6221e335447e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = output_dir / \"flickr_train_top1.parquet\"\n",
    "filepath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f0eabc-d301-491c-90c8-fc542623670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Flicker30K(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e7bbbc-1c13-42ce-af92-424449ad4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(filepath, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d176c478-2fd8-44cd-b9f1-41a4f607f32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_emb': tensor([ 4.3015e-01,  4.9582e-01, -8.4673e-02, -1.6892e-02,  1.1948e-01,\n",
       "         -2.8855e-01,  3.1858e-01,  3.3653e-01,  8.4516e-02,  1.9869e-02,\n",
       "          2.0031e-01, -3.0132e-02,  3.3788e-01, -1.6115e-01, -1.3101e-01,\n",
       "          7.2758e-02, -8.4261e-01,  4.3193e-01, -5.2723e-02, -6.8167e-02,\n",
       "          1.0022e+00, -9.2721e-02,  7.1727e-02, -1.4961e-01,  3.3231e-01,\n",
       "         -4.2866e-01,  1.8314e-01,  3.8839e-01,  2.4105e-01, -2.4624e-01,\n",
       "         -1.0893e-03,  2.7882e-02, -2.8714e-02,  8.9607e-02,  2.0518e-01,\n",
       "          1.3776e-01, -1.1391e-01, -1.6325e-01,  4.9538e-01, -1.2830e-01,\n",
       "         -6.9628e-02, -5.2208e-02, -1.5452e-01, -3.8287e-01,  2.9060e-01,\n",
       "         -7.2306e-01, -4.4488e-02,  2.2103e-02,  3.7364e-02,  8.6819e-02,\n",
       "          2.0576e-01,  3.9557e-01, -5.1237e-02, -3.1715e-01,  1.0678e-01,\n",
       "          2.2878e-01,  3.4304e-02,  1.8025e-01, -3.7274e-01, -1.3906e-01,\n",
       "         -4.7516e-02, -1.9967e-01,  1.8347e-01,  1.7822e-01,  1.7647e-01,\n",
       "         -2.1175e-01, -2.4432e-02,  1.3108e+00, -4.1046e-02, -2.8708e-01,\n",
       "         -6.7513e-03,  4.7970e-02, -6.0130e-01,  5.4948e-02,  1.5938e-01,\n",
       "         -1.7051e-01, -7.7351e-01, -2.2460e-01,  1.6460e-01, -3.5255e-01,\n",
       "          3.4097e-01,  4.2442e-01,  2.6393e-02,  1.5823e-01, -3.9735e-01,\n",
       "          2.0596e-01,  8.2861e-01,  3.6400e-01, -5.6588e-01, -4.3494e-01,\n",
       "         -4.6662e-02,  8.4897e-02, -7.4136e+00, -1.8565e-01, -1.7760e-01,\n",
       "         -8.5686e-02,  3.4815e-01,  1.1189e-01, -5.8437e-01,  4.7837e-01,\n",
       "         -8.0312e-02,  1.5987e-02, -1.4356e-01,  1.5288e-01,  6.8374e-01,\n",
       "          3.7761e-01, -3.1556e-01, -4.1397e-01,  3.1724e-02, -1.3254e-01,\n",
       "          3.2385e-01, -5.9757e-02, -6.1729e-01, -3.6240e-01, -1.7358e-01,\n",
       "         -1.9814e-02, -1.3704e-01,  2.6083e-01,  3.4275e-01, -5.3772e-01,\n",
       "          3.6862e-02, -2.0878e-01, -3.9128e-01, -2.2260e-01,  1.4345e-01,\n",
       "          2.6308e-01, -3.0096e-02,  2.1411e-01, -2.1241e-01,  2.4851e-01,\n",
       "          5.3209e-01,  5.7230e-01, -3.5932e-01,  1.0370e+00, -8.8496e-02,\n",
       "         -2.2889e-02,  7.3132e-02,  1.3339e-02, -6.7987e-02, -3.3896e-01,\n",
       "          2.0393e-01, -4.4073e-02,  4.6414e-02,  2.1656e-01, -6.9902e-02,\n",
       "          2.7170e-01, -9.7093e-03,  1.4229e-01, -4.4695e-01,  1.1117e-01,\n",
       "          4.1001e-01, -5.9870e-01,  6.2187e-01, -1.4347e-01, -5.0094e-03,\n",
       "         -2.2743e-01, -2.1538e-02,  2.1950e-01,  2.9201e-01, -4.1812e-01,\n",
       "         -4.5062e-01, -6.1857e-01, -2.7846e-01, -8.8866e-02,  7.1669e-01,\n",
       "          4.2979e-02,  9.5362e-02,  1.9396e-01,  2.4520e-01, -1.2006e-01,\n",
       "         -1.1202e-01, -1.9511e-01, -9.3876e-02, -1.9186e-01,  3.1497e-01,\n",
       "         -1.8402e-01,  1.3523e+00, -6.4303e-02, -6.7368e-01, -9.4775e-02,\n",
       "         -3.2400e-01, -1.2690e-01, -5.3519e-02, -9.0630e-02,  5.6786e-02,\n",
       "         -1.7236e-01,  5.9283e-01,  3.8307e-01,  1.1775e-02,  7.3924e-02,\n",
       "          4.5699e-02, -1.8506e-01,  2.3247e-01,  2.8717e-01, -1.0838e+00,\n",
       "         -1.0048e-02, -2.8549e-02,  9.2975e-02,  1.4476e-02,  1.4189e-01,\n",
       "         -2.1060e-01,  2.0195e-01,  4.7351e-01, -3.8027e-02,  2.9194e-01,\n",
       "          2.5666e-01, -2.1483e-01,  1.9693e-01,  4.4446e-01,  3.4529e-01,\n",
       "         -8.5167e-01,  3.2295e-01, -7.5064e-02, -4.5837e-01, -6.4985e-02,\n",
       "         -9.7949e-02,  4.2435e-02, -1.4649e-01,  1.0793e+00, -2.2984e-01,\n",
       "         -2.2596e-01, -1.9875e-01, -1.1436e-01, -5.7869e-01, -2.3459e-01,\n",
       "          1.1646e-01, -4.6074e-01, -3.3875e-01,  8.7597e-02, -1.2746e-01,\n",
       "          1.7427e-01, -4.0146e-01, -1.9163e-01, -2.5283e-01, -8.4287e-02,\n",
       "         -5.5860e-02, -3.2450e-01,  3.6075e-02, -1.6732e-01, -3.8887e-02,\n",
       "          1.6429e-01,  2.4169e-01,  5.2766e-02,  1.1283e-01, -5.0088e-01,\n",
       "         -2.1537e-01, -2.4552e-01,  5.0890e-01, -2.0833e-01, -7.8214e-02,\n",
       "         -1.9748e-01, -2.6611e-02,  2.3879e-01,  1.7930e-01,  4.2983e-01,\n",
       "         -2.0703e-02, -7.9635e-02, -1.5765e-01,  9.8145e-01,  3.2220e-01,\n",
       "         -3.8845e-01, -1.3674e-01, -1.4852e-01, -4.8591e-01,  1.1505e-01,\n",
       "         -1.3049e-01,  8.6362e-02,  1.2719e-01,  2.8170e-01, -5.9002e-02,\n",
       "          1.5099e-01, -7.8669e-02,  2.9795e-01,  5.7024e-01,  7.6995e-02,\n",
       "         -2.5223e-01, -1.4534e-01, -1.4459e-01, -2.1486e-01, -1.7852e-01,\n",
       "         -2.8650e-02, -1.3870e-02, -8.9766e-01,  4.1798e-01, -8.5672e-02,\n",
       "          2.0338e-01, -9.4550e-02, -4.0460e-02, -4.2597e-01, -8.9444e-02,\n",
       "          2.5177e-02,  1.4629e-01, -3.1983e-01, -2.4109e-01, -1.3637e-01,\n",
       "         -2.9536e-01,  4.6263e-01, -4.7333e-01,  2.1439e-01, -2.9635e-01,\n",
       "          2.5023e-02, -1.8164e-01,  1.6765e-01,  4.9164e-02,  1.8062e-01,\n",
       "         -6.1744e-01,  3.9363e-03,  1.7400e-02,  6.6526e-01, -1.9830e-01,\n",
       "         -1.0675e-01, -3.2381e-01,  1.0361e+00,  2.4821e-01,  4.6390e-02,\n",
       "          3.3139e-01,  1.8719e-01,  1.0346e+00, -4.2240e-01,  2.4320e-02,\n",
       "          3.5512e-01,  5.1644e-01, -3.4706e-01,  1.0593e-02, -9.1222e-02,\n",
       "         -9.7621e-02, -1.0634e-01,  2.8812e-01,  3.7236e-02,  5.4189e-01,\n",
       "         -2.6417e-01,  1.8688e-01,  1.1320e-01, -7.8117e-01, -3.0468e-01,\n",
       "          2.3030e-01,  9.5336e-02,  1.2914e-01,  2.0926e-01, -5.7827e-02,\n",
       "          6.7527e-02, -3.2196e-01,  4.2063e-02, -2.7868e-01, -1.4126e-01,\n",
       "         -2.2705e-01,  3.1772e-02, -5.5665e-02,  6.2352e-02, -1.8143e-01,\n",
       "          4.5295e-01,  9.6653e-02,  1.9722e-01, -3.3368e-01, -2.5287e-01,\n",
       "          1.3077e-01,  2.2324e-03, -2.8774e-01,  1.5173e-01, -2.7702e-02,\n",
       "         -6.1899e-01, -2.2829e-01,  3.7276e-01, -6.5051e-02, -3.1461e-01,\n",
       "         -3.7687e-01, -1.5278e-01, -3.4428e-01, -9.7285e-02, -7.7728e-03,\n",
       "         -3.0249e-01,  6.6261e-01, -8.8041e-02, -8.0715e-02, -7.1078e-02,\n",
       "         -6.6965e-03,  1.8362e+00,  5.5631e-02,  2.9198e-01,  3.4630e-01,\n",
       "         -6.1470e-02, -3.0440e-02, -3.8092e-01, -2.5691e-01, -1.8671e-01,\n",
       "         -4.2679e-01,  5.0297e-01,  5.4440e-02, -1.5763e-01,  4.0620e-01,\n",
       "         -6.9223e-01, -2.5028e-01,  5.0478e-01, -1.4482e-01,  1.4704e-01,\n",
       "          1.9487e-01, -3.2609e-02,  6.3931e-02,  3.4180e-02,  1.7454e-02,\n",
       "         -1.9512e-01, -9.1484e-03,  5.8409e-01, -2.2778e-01, -2.2127e-02,\n",
       "          7.4101e-03, -2.4557e-01,  1.5966e-03,  2.2898e-01, -1.2321e-01,\n",
       "         -1.3101e+00, -6.7251e-03, -1.9330e-01,  8.2169e-02,  2.5377e-01,\n",
       "          1.7532e-01,  1.5202e-01,  7.3091e-02, -7.2939e-01,  1.0982e-01,\n",
       "         -4.5333e-02, -7.2486e-02,  2.9809e-01, -8.6536e-02,  3.4746e-02,\n",
       "          5.1664e-01, -2.3049e-01, -9.6696e-02, -4.9333e-01,  2.1065e-01,\n",
       "          2.8370e-01,  4.3640e-01,  8.4782e-01, -3.3255e-01,  3.4572e-01,\n",
       "          2.0709e-02, -3.5776e-01, -8.2192e-03,  2.8154e-01, -3.5100e-01,\n",
       "          2.2511e-02, -1.2601e-01, -2.6355e-01,  2.2493e-02, -3.2546e-01,\n",
       "         -1.9322e-01,  1.9834e-01, -7.7761e-03, -4.1413e-01,  1.4151e-01,\n",
       "         -8.2593e-01, -4.7515e-02,  2.6526e-01, -2.2137e-01, -5.0925e-02,\n",
       "         -1.9403e-01, -2.6604e-01, -4.0119e-01,  3.6983e-01, -2.2724e-02,\n",
       "          2.8491e-01, -2.4518e-01,  1.7772e-01,  3.7710e-02,  3.0116e-01,\n",
       "         -1.5791e-01,  6.2530e-02,  1.8164e-01, -3.4076e-01,  3.6652e-02,\n",
       "          5.2833e-02, -3.9432e-01, -1.0972e-01, -3.0925e-01, -4.4336e-02,\n",
       "         -2.8654e-01,  2.9509e-01, -2.4757e-01, -2.0150e-01, -4.4372e-01,\n",
       "          8.5029e-02, -3.5335e-01,  1.2124e-01, -3.4696e-01,  2.6359e-01,\n",
       "          1.9743e-01,  6.5159e-02,  1.9801e-01, -2.3573e-02, -5.1413e-01,\n",
       "          2.9614e-01, -2.2966e-01, -1.0767e-01, -4.8891e-01, -7.4998e-02,\n",
       "          6.6551e-02,  3.2621e-01,  7.5650e-01, -1.1046e-01,  4.0837e-01,\n",
       "         -3.8588e-02,  2.6294e-01, -2.3990e-01, -3.0942e-01,  6.2948e-01,\n",
       "         -5.6717e-01, -1.0825e-01, -1.1997e-01, -2.6762e-01,  5.8264e-01,\n",
       "         -1.6366e-01, -2.5282e-02]),\n",
       " 'caption': 'Two men in green shirts are standing in a yard.',\n",
       " 'img_id': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81f6753b-6d90-4c2d-9953-534c435da330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_embedding</th>\n",
       "      <th>caption_text</th>\n",
       "      <th>caption_embedding</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>caption_tokens</th>\n",
       "      <th>img_id</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.4301545321941376, 0.49581533670425415, -0.0...</td>\n",
       "      <td>Two men in green shirts are standing in a yard.</td>\n",
       "      <td>[[0.3392859101295471, 0.11646018177270889, 0.1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[49406, 1237, 1656, 530, 1901, 5803, 631, 2862...</td>\n",
       "      <td>0</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.29502591490745544, 0.3597133755683899, -0.2...</td>\n",
       "      <td>Several men in hard hats are operating a giant...</td>\n",
       "      <td>[[0.3392859101295471, 0.11646018177270889, 0.1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[49406, 5560, 1656, 530, 1626, 9812, 631, 1221...</td>\n",
       "      <td>1</td>\n",
       "      <td>10002456.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.16267558932304382, 0.13509173691272736, 0.0...</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>[[0.3392859101295471, 0.11646018177270889, 0.1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[49406, 320, 1274, 1611, 530, 320, 3360, 2595,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1000268201.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.28787654638290405, 0.18737243115901947, -0....</td>\n",
       "      <td>Man in blue shirt and jeans on ladder cleaning...</td>\n",
       "      <td>[[0.3392859101295471, 0.11646018177270889, 0.1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[49406, 786, 530, 1746, 2523, 537, 10157, 525,...</td>\n",
       "      <td>3</td>\n",
       "      <td>1000344755.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.17353691160678864, 0.33811840415000916, 0....</td>\n",
       "      <td>Two men, one in a gray shirt, one in a black s...</td>\n",
       "      <td>[[0.3392859101295471, 0.11646018177270889, 0.1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[49406, 1237, 1656, 267, 637, 530, 320, 7048, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1000366164.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       img_embedding  \\\n",
       "0  [0.4301545321941376, 0.49581533670425415, -0.0...   \n",
       "1  [0.29502591490745544, 0.3597133755683899, -0.2...   \n",
       "2  [0.16267558932304382, 0.13509173691272736, 0.0...   \n",
       "3  [0.28787654638290405, 0.18737243115901947, -0....   \n",
       "4  [-0.17353691160678864, 0.33811840415000916, 0....   \n",
       "\n",
       "                                        caption_text  \\\n",
       "0    Two men in green shirts are standing in a yard.   \n",
       "1  Several men in hard hats are operating a giant...   \n",
       "2  A little girl in a pink dress going into a woo...   \n",
       "3  Man in blue shirt and jeans on ladder cleaning...   \n",
       "4  Two men, one in a gray shirt, one in a black s...   \n",
       "\n",
       "                                   caption_embedding  \\\n",
       "0  [[0.3392859101295471, 0.11646018177270889, 0.1...   \n",
       "1  [[0.3392859101295471, 0.11646018177270889, 0.1...   \n",
       "2  [[0.3392859101295471, 0.11646018177270889, 0.1...   \n",
       "3  [[0.3392859101295471, 0.11646018177270889, 0.1...   \n",
       "4  [[0.3392859101295471, 0.11646018177270889, 0.1...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                      caption_tokens  img_id        filename  \n",
       "0  [49406, 1237, 1656, 530, 1901, 5803, 631, 2862...       0  1000092795.jpg  \n",
       "1  [49406, 5560, 1656, 530, 1626, 9812, 631, 1221...       1    10002456.jpg  \n",
       "2  [49406, 320, 1274, 1611, 530, 320, 3360, 2595,...       2  1000268201.jpg  \n",
       "3  [49406, 786, 530, 1746, 2523, 537, 10157, 525,...       3  1000344755.jpg  \n",
       "4  [49406, 1237, 1656, 267, 637, 530, 320, 7048, ...       4  1000366164.jpg  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cdeb0b49-1983-4aa9-b8a6-00e251602f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb, cap, cap_emb, attention_mask, cap_tokens = df.iloc[0][\n",
    "            [\n",
    "                \"img_embedding\",\n",
    "                \"caption_text\",\n",
    "                \"caption_embedding\",\n",
    "                \"attention_mask\",\n",
    "                \"caption_tokens\",\n",
    "            ]\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8b485-ac7a-4c8d-ad46-c3d516c67009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10353cfb-5ce9-4032-94b9-62499cb0f26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(cap_emb[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebfd360f-7277-4e16-9b23-d5be6e574e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8a03e0f-126c-4fe2-bc16-a77bb73ae2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array(cap_emb.tolist())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b038385-c110-4fcb-a470-80ee54271a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77,)\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "print(cap_emb.shape)\n",
    "print(cap_emb[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
